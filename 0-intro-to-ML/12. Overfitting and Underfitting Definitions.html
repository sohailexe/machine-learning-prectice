<p>Before we get into the experimentation side of things, it's worth having a little reminder of overfitting and underfitting are.</p><p>All experiments should be conducted on different portions of your data.</p><ul><li><p><strong>Training data set</strong> — Use this set for model training, 70–80% of your data is the standard.</p></li><li><p><strong>Validation/development data set</strong> — Use this set for model hyperparameter tuning and experimentation evaluation, 10–15% of your data is the standard.</p></li><li><p><strong>Test data set</strong> — Use this set for model testing and comparison, 10–15% of your data is the standard.</p></li></ul><p>These amounts can fluctuate slightly, depending on your problem and the data you have.</p><p>Poor performance on training data means the model hasn’t learned properly and is <strong>underfitting</strong>. Try a different model, improve the existing one through hyperparameter or collect more data.</p><p>Great performance on the training data but poor performance on test data means your model doesn’t generalize well. Your model may be <strong>overfitting</strong> the training data. Try using a simpler model or making sure your the test data is of the same style your model is training on.</p><p>Another form of <strong>overfitting</strong> can come in the form of better performance on test data than training data. This may mean your testing data is leaking into your training data (incorrect data splits) or you've spent too much time optimizing your model for the test set data. Ensure your training and test datasets are kept separate at all times and avoid optimizing a models performance on the test set (use the training and validation sets for model improvement).</p><p>Poor performance once deployed (in the real world) means there’s a difference in what you trained and tested your model on and what is actually happening. Ensure the data you're using during experimentation matches up with the data you're using in production.</p>